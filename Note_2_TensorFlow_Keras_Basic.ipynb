{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Note_1_TensorFlow_Datasets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/Davidxswang/ML/blob/master/Note_1_TensorFlow_Datasets.ipynb",
      "authorship_tag": "ABX9TyMLAkubH24fzAPNiOmGO+24",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Davidxswang/ML/blob/master/Note_2_TensorFlow_Keras_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNBJth7oyPKb",
        "colab_type": "text"
      },
      "source": [
        "# Import the Packages and Check the Environment\n",
        "[Refer to the link for full tutorial from TensorFlow](https://www.tensorflow.org/datasets/overview)\n",
        "\n",
        "**Be careful, in the tutorial, two lines of code cannot work:**\n",
        "- **fig = tfds.show_examples(ds, info)**\n",
        "- **print(info.splits['train'].filenames)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wer7IfYTQEC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY3vNJhTZy4b",
        "colab_type": "code",
        "outputId": "15f56e7f-c932-4b53-c32c-7049c35b354e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! pwd\n",
        "# mnist dataset is stored in the path below\n",
        "#! mkdir /content/drive/My\\ Drive/colab/mnist"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFEpOR6Eybfd",
        "colab_type": "text"
      },
      "source": [
        "# How to Use tfds\n",
        "\n",
        "## See What Datasets are Included in tfds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NDidZVSY-2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To see what datasets are available (included in tfds)\n",
        "if False:\n",
        "  tfds.list_builders()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq-FqIFiyqGH",
        "colab_type": "text"
      },
      "source": [
        "## How to Load the Dataset, What Type of Structure the Data Has\n",
        "\n",
        "There are three key things you need to know:\n",
        "1. as_supervised argument of load method\n",
        "2. batch_size argument of load method\n",
        "3. as_numpy() method of tdfs\n",
        "\n",
        "The 1 and 2 will affect the output structure of the load method.\n",
        "\n",
        "The 3 can convert the **tf.data.Dataset** into **Generator[np.ndarray]** and **tf.Tensor** into **np.ndarray**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wc1sk2PTOBM",
        "colab_type": "code",
        "outputId": "1cb27289-2864-4958-c18b-efd5561ccb55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# all the configs\n",
        "as_supervised = True\n",
        "# if True, elements in ds_train are tuple, if False, elements are dict\n",
        "\n",
        "batch_size = None\n",
        "# -1, load the full batch into a tuple/dict, ds_train will be a tuple if used with as_supervised=True, a dict if as_supervised=False\n",
        "# None, ds_train will be tf.data.Dataset, no matter what as_supervised is\n",
        "\n",
        "\n",
        "# load the data\n",
        "# shuffle_files can be useful when the dataset stores the data in multiple files, being set to True will let the program read from the files randomly.\n",
        "# This will result in better randomness. Otherwise, it's not truly randomized because the files will be read in the same order.\n",
        "# MNIST only have 1 file as you can see from the output of this cell, so it doesn't really matter in MNIST case.\n",
        "\n",
        "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train[:80%]', 'train[80%:]', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=as_supervised,     \n",
        "    with_info=True,\n",
        "    data_dir='/content/drive/My Drive/colab/mnist',\n",
        "    batch_size=batch_size\n",
        ")\n",
        "print(ds_info)\n",
        "\n",
        "print('We can access the features of the dataset by this way:')\n",
        "print(ds_info.features.shape, ds_info.features.dtype)\n",
        "print(ds_info.features['image'].shape, ds_info.features['image'].dtype)\n",
        "print(ds_info.features['label'].num_classes, ds_info.features['label'].names, ds_info.features['label'].int2str(7), ds_info.features['label'].str2int('7'))\n",
        "print(\"\\n\")\n",
        "\n",
        "print('We can access the split of the dataset by this way:')\n",
        "print(ds_info.splits)\n",
        "print(list(ds_info.splits.keys()))\n",
        "print(dir(ds_info.splits['train']))\n",
        "print(ds_info.splits['train'].num_examples)\n",
        "print(ds_info.splits['train'].file_instructions)\n",
        "print(ds_info.splits['train'].file_instructions[0]['filename'])\n",
        "print(ds_info.splits['train'].num_shards)\n",
        "print(ds_info.splits['train[15%:25%]'].num_examples)\n",
        "print(ds_info.splits['train[15%:25%]'].file_instructions)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('ds_train type is (tuple, dict or tf.data.Dataset): ', type(ds_train))\n",
        "# take a look at the internal structure of the data\n",
        "if batch_size == -1:\n",
        "  # ds_train will be dict or tuple\n",
        "  if as_supervised:\n",
        "    print('ds_train will be a tuple (images, labels)')\n",
        "    print('How many images in ds_train: ', len(ds_train[0]))\n",
        "    print('Type of elements in ds_train\\'s images: ', type(ds_train[0][0]))\n",
        "    print('Shape of the image: ', ds_train[0][0].shape)\n",
        "    print('Type of elements in ds_train\\'s labels: ', type(ds_train[1][0]))\n",
        "    print('The value of the label: ', ds_train[1][0].numpy())\n",
        "    print('\\n')\n",
        "    print('We can use tfds.as_numpy(something) to convert something into: Generator[np.ndarray] from tf.data.Dataset, or np.ndarray from tf.Tensor')\n",
        "    print('We can convert ds_train[0] which is originally a tf.Tensor, now the type is: ', type(tfds.as_numpy(ds_train[0])))\n",
        "    print('We can also convert the ds_train (a tuple) directly, after convert it\\'s still a', type(tfds.as_numpy(ds_train)), ', the ds_train[0] is now the type of: ', type(tfds.as_numpy(ds_train)[0]))\n",
        "  else:\n",
        "    print('ds_train will be a dict {\\'image\\': image, \\'label\\': label}')\n",
        "    print('Type of ds_train[\\'image\\']: ', type(ds_train['image']))\n",
        "    print('Shape of ds_train[\"image\"]: ', ds_train['image'].shape)\n",
        "    print('Type of ds_train[\\'label\\']: ', type(ds_train['label']))\n",
        "    print('Shape of ds_train[\"label\"]: ', ds_train['label'].shape)\n",
        "    print('\\n')\n",
        "    print('We can convert ds_train[\"image\"] which is originally a tf.Tensor, now the type is: ', type(tfds.as_numpy(ds_train[\"image\"])))\n",
        "    print('We can also convert the ds_train (a dict) directly, after convert it\\'s still a', type(tfds.as_numpy(ds_train)), ', the ds_train[\"image\"] is now the type of: ', type(tfds.as_numpy(ds_train)[\"image\"]))\n",
        "elif batch_size is None:\n",
        "  print('ds_train is a tf.data.Dataset object')\n",
        "  ds = ds_train.take(1)\n",
        "  print('Use element_spec to see the structure of element in the dataset', ds_train.element_spec)\n",
        "  for data in ds:\n",
        "    if as_supervised:\n",
        "      print('The element in ds_train is tuple, as you can see: ', type(data))\n",
        "      print('The type of the first element of this tuple: ', type(data[0]))\n",
        "      print('The shape of this image (tf.Tensor): ', data[0].shape)\n",
        "      print('The type of the second element of this tuple: ', type(data[1]))\n",
        "      print('The shape of this label (tf.Tensor): ', data[1].shape)\n",
        "      print('The value of this label (tf.Tensor): ', data[1].numpy())\n",
        "      print('\\n')\n",
        "      print('We can conver the tf.data.Dataset using tfds.as_numpy() method, the type will be: ', type(tfds.as_numpy(ds)))\n",
        "      print('The element from this generator will be the type of tuple: ', type(list(tfds.as_numpy(ds))[0]))\n",
        "      print('The first element (an image) of this tuple will be the type of np.ndarray: ', type(list(tfds.as_numpy(ds))[0][0]))\n",
        "      print('The shape of this image will be: ', list(tfds.as_numpy(ds))[0][0].shape)\n",
        "    else:\n",
        "      print('The element in ds_train is dict, as you can see: ', type(data))\n",
        "      print('The keys are: ', list(data.keys()))\n",
        "      print('The type of the first element of this dict: ', type(data['image']))\n",
        "      print('The shape of this image (tf.Tensor): ', data['image'].shape)\n",
        "      print('The type of the second element of this dict: ', type(data['label']))\n",
        "      print('The shape of this label (tf.Tensor): ', data['label'].shape)\n",
        "      print('The value of this label (tf.Tensor): ', data['label'].numpy())\n",
        "      print('\\n')\n",
        "      print('We can conver the tf.data.Dataset using tfds.as_numpy() method, the type will be: ', type(tfds.as_numpy(ds)))\n",
        "      print('The element from this generator will be the type of dict: ', type(list(tfds.as_numpy(ds))[0]))\n",
        "      print('The image of this dict will be the type of np.ndarray: ', type(list(tfds.as_numpy(ds))[0]['image']))\n",
        "      print('The shape of this image will be: ', list(tfds.as_numpy(ds))[0]['image'].shape)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='mnist',\n",
            "    version=3.0.0,\n",
            "    description='The MNIST database of handwritten digits.',\n",
            "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
            "    }),\n",
            "    total_num_examples=70000,\n",
            "    splits={\n",
            "        'test': 10000,\n",
            "        'train': 60000,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"@article{lecun2010mnist,\n",
            "      title={MNIST handwritten digit database},\n",
            "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
            "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
            "      volume={2},\n",
            "      year={2010}\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n",
            "We can access the features of the dataset by this way:\n",
            "{'image': (28, 28, 1), 'label': ()} {'image': tf.uint8, 'label': tf.int64}\n",
            "(28, 28, 1) <dtype: 'uint8'>\n",
            "10 ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] 7 7\n",
            "\n",
            "\n",
            "We can access the split of the dataset by this way:\n",
            "{'test': <tfds.core.SplitInfo num_examples=10000>, 'train': <tfds.core.SplitInfo num_examples=60000>}\n",
            "['test', 'train']\n",
            "['_ProtoCls__proto', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_dataset_name', 'file_instructions', 'get_proto', 'num_examples']\n",
            "60000\n",
            "[{'filename': 'mnist-train.tfrecord-00000-of-00001', 'skip': 0, 'take': -1}]\n",
            "mnist-train.tfrecord-00000-of-00001\n",
            "10\n",
            "6000\n",
            "[{'filename': 'mnist-train.tfrecord-00000-of-00001', 'skip': 9000, 'take': 6000}]\n",
            "\n",
            "\n",
            "ds_train type is (tuple, dict or tf.data.Dataset):  <class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n",
            "ds_train is a tf.data.Dataset object\n",
            "Use element_spec to see the structure of element in the dataset (TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n",
            "The element in ds_train is tuple, as you can see:  <class 'tuple'>\n",
            "The type of the first element of this tuple:  <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "The shape of this image (tf.Tensor):  (28, 28, 1)\n",
            "The type of the second element of this tuple:  <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "The shape of this label (tf.Tensor):  ()\n",
            "The value of this label (tf.Tensor):  1\n",
            "\n",
            "\n",
            "We can conver the tf.data.Dataset using tfds.as_numpy() method, the type will be:  <class 'generator'>\n",
            "The element from this generator will be the type of tuple:  <class 'tuple'>\n",
            "The first element (an image) of this tuple will be the type of np.ndarray:  <class 'numpy.ndarray'>\n",
            "The shape of this image will be:  (28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtgS8uMhzb5p",
        "colab_type": "text"
      },
      "source": [
        "## How to Use the Dataset to Train a Neural Network\n",
        "[Refer to the full tutorial from TensorFlow](https://www.tensorflow.org/datasets/keras_example)\n",
        "\n",
        "### Build Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWUgHpBAzr4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to normalize the images from tf.uint8 to tf.float32\n",
        "# This method will be used in the map method of a tf.data.Dataset\n",
        "# When being used, every element in the Dataset will be used an the input to this function. As we can see from the cell above, every element is a tuple (image, label)\n",
        "# So after using this function, the images in Dataset will be converted to tf.float32 and normalized to [0,1], label will be kept as it is.\n",
        "def normalize_img(image, label):\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "# When specifying the num_parallel_calls, the system will choose how many parallels calls based on the available CPU\n",
        "# The input signature of map_func is determined by the structure of each element in this dataset.\n",
        "# determinitic argument controls whether determinism should be traded for performance by allowing elements to be produced out of order. \n",
        "# If deterministic is None, the tf.data.Options.experimental_deterministic dataset option (True by default) is used to decide whether to produce elements deterministically.\n",
        "ds_train = ds_train.map(map_func=normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=None)\n",
        "\n",
        "# Cache is a very useful function to improve the performance of the training.\n",
        "# The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory(we can assign the cache location in the argument).\n",
        "# Subsequent iterations will use the cached data.\n",
        "# Since we want to shuffle the data before training, it's better to normalize the data and cache the data right after normalization, especially before shuffle.\n",
        "# In this way, when we are training the network, it will take the samples from the cache, then shuffle. This will make sure the data has been shuffled to fit in training.\n",
        "# Quote from TensorFlow Dataset Tutorial: Random transformations should be applied after caching and batching.\n",
        "# For small datasets, tfds will automatically cache the dataset. But we need to refer to that specific dataset doc page. Refer to: https://www.tensorflow.org/datasets/performances\n",
        "# Large dataset are sharded, they usually don't fit in memory, so they should not be cached.\n",
        "ds_train = ds_train.cache()\n",
        "\n",
        "# When shuffle is enabled, this dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. \n",
        "# For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required. \n",
        "# Suggestions from TF: For bigger datasets which do not fit in memory, a standard value is 1000 if your system allows it.\n",
        "# The tf.data.Dataset objects are Python iterables.\n",
        "# So if we want each time when we iterate over Dataset, the Dataset gives us a different ordered data, we can set reshuffle_each_iteration to True.\n",
        "# If we set reshuffle_each_iteration, every iteration of the dataset, the order will be the same. Default to True.\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples,seed=None,reshuffle_each_iteration=True)\n",
        "\n",
        "# Batch will combine consecutive elements of this dataset into batches. Batch after shuffling, we can get unique batches at each epoch.\n",
        "# If drop_remainder is True, then the remainder of the dataset after batching will be ignored. Default to False.\n",
        "ds_train = ds_train.batch(128,drop_remainder=True)\n",
        "\n",
        "# Data augmentation method, to randomly change the brightness and contrast\n",
        "# If the application is eligible, we can also left-right flip or top-bottom flip (if images allow), or randomly change the saturation (RGB) and so on\n",
        "# TensorFlow recommend us perform data augmentation after batching, because a lot of data augmentation methods can receive images in 4D shape\n",
        "# If it's batched, the system will perform data augmentation in parallel\n",
        "# More detail about randomly changing brightness/contrast and so on will be talked about later.\n",
        "def augmentation(image, label):\n",
        "  image = tf.image.random_brightness(image, 0.2)\n",
        "  image = tf.image.random_contrast(image, 0.2, 0.5)\n",
        "  return image, label\n",
        "\n",
        "ds_train = ds_train.map(augmentation, num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=None)\n",
        "\n",
        "# Good practice to end the pipeline by prefetching for performance.\n",
        "# Prefetching overlaps the preprocessing and model execution of a training step.\n",
        "# Prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. \n",
        "# examples.prefetch(2) will prefetch two elements (2 examples)\n",
        "# examples.batch(20).prefetch(2) will prefetch 2 elements (in this case, 2 batches, of 20 examples each).\n",
        "# The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step.\n",
        "# You could either manually tune this value, or set it to tf.data.experimental.AUTOTUNE which will prompt the tf.data runtime to tune the value dynamically at runtime.\n",
        "# Refer to: https://www.tensorflow.org/guide/data_performance#prefetching\n",
        "# Refer to: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch\n",
        "ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XpZJ4IX-MxM",
        "colab_type": "text"
      },
      "source": [
        "### Build Validation and Testing Pipeline\n",
        "\n",
        "Since shuffle has no effect on test/validation accuracy, shuffle is not performed on test set and validation set.\n",
        "\n",
        "Since we are not going to shuffle them, we can cache the batches directly. Each time we can just yank out the batches and test them directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0sfdLJU-M9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Validation set.\n",
        "ds_val = ds_val.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_val = ds_val.batch(128, drop_remainder=True)\n",
        "ds_val = ds_val.cache()\n",
        "\n",
        "ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Test set.\n",
        "ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test.batch(128, drop_remainder=True)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Oh0F2YxA3mt",
        "colab_type": "text"
      },
      "source": [
        "### Now Train the Model\n",
        "\n",
        "The model is not the important topic in this note. So not much explanation about model in this note.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf2BEdLSA3vj",
        "colab_type": "code",
        "outputId": "653afaff-0090-41c1-ac41-210d70202bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "model.summary()\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=10,\n",
        "    validation_data=ds_val,\n",
        "    )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.8121 - accuracy: 0.7835 - val_loss: 0.5377 - val_accuracy: 0.8831\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.3943 - accuracy: 0.8916 - val_loss: 0.5002 - val_accuracy: 0.9068\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.3540 - accuracy: 0.9006 - val_loss: 0.4724 - val_accuracy: 0.9157\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.3034 - accuracy: 0.9150 - val_loss: 0.4431 - val_accuracy: 0.9246\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.2864 - accuracy: 0.9181 - val_loss: 0.4175 - val_accuracy: 0.9304\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.2440 - accuracy: 0.9325 - val_loss: 0.3722 - val_accuracy: 0.9373\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.2236 - accuracy: 0.9374 - val_loss: 0.3486 - val_accuracy: 0.9409\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.2000 - accuracy: 0.9443 - val_loss: 0.3116 - val_accuracy: 0.9480\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.2209 - accuracy: 0.9354 - val_loss: 0.3294 - val_accuracy: 0.9435\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.2179 - accuracy: 0.9382 - val_loss: 0.2935 - val_accuracy: 0.9516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyut6AQ4BZKC",
        "colab_type": "code",
        "outputId": "e10b00ae-b8c3-4495-b7d9-80809c5f7c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(ds_test)\n",
        "print(history)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78/78 [==============================] - 2s 19ms/step - loss: 0.2593 - accuracy: 0.9561\n",
            "<tensorflow.python.keras.callbacks.History object at 0x7f1adc396358>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}